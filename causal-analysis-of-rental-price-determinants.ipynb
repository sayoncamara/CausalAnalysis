{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":4924616,"sourceType":"datasetVersion","datasetId":2855778}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/sayoncamara/causal-analysis-of-rental-price-determinants?scriptVersionId=204506595\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"#  Causal Analysis of Rental Price Determinants: Policy Insights for Real Estate Interventions in Brussels\n\n1. **Introduction**\n\nIn today's dynamic real estate market, understanding the factors that influence rental prices is critical for property owners, investors, and policymakers. Traditional models often provide predictions based on correlations, but they may not fully capture the causal relationships between different property features and rental prices. This project takes a step further by employing causal inference techniques to determine not only which features are most important in predicting rental prices but also how interventions like adding amenities or making structural improvements can impact rental returns.\n\nThe project begins with extensive exploratory data analysis (EDA), as the dataset used is notably dirty, with a significant portion of missing values. Careful imputation techniques were employed to handle these gaps and prepare the data for analysis. After cleaning the data, a monthly rental price prediction model was built to extract the most important predictive features using SHAP values, which helped highlight the top-ranking variables that contribute to rental price predictions.\n\nHowever, while SHAP values reveal feature importance in a predictive sense, they do not provide insights into the causal effects of these features. To bridge this gap, we leverage CausalAnalysis from EconML to determine which features not only correlate with higher rents but also causally impact rental prices. By understanding the actual causal relationships, we can provide more reliable insights into how specific property features influence rental income.\n\nIn the next phase, we explore how property features—such as heating systems, solar panels, furnishing, and building conditions—affect rental prices through causal inference models. Using tools like policy trees and heterogeneity analysis, we dive deep into how these features impact different types of properties.\n\nUltimately, this project aims to offer actionable insights for real estate decision-makers, highlighting which property features have the greatest potential to increase rental prices and guiding cost-effective property improvements.","metadata":{}},{"cell_type":"markdown","source":"2. **Data exploration**","metadata":{}},{"cell_type":"code","source":"\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.preprocessing import LabelEncoder\n\n# Set pandas option to display all columns\npd.set_option('display.max_columns', None)\n\n# Load your dataset\ndata = pd.read_csv('/kaggle/input/monthly-rent-of-rented-flats-in-brussels/rental_df_dirty.csv')\n","metadata":{"execution":{"iopub.status.busy":"2024-10-30T15:49:08.044153Z","iopub.execute_input":"2024-10-30T15:49:08.044516Z","iopub.status.idle":"2024-10-30T15:49:09.096343Z","shell.execute_reply.started":"2024-10-30T15:49:08.04448Z","shell.execute_reply":"2024-10-30T15:49:09.094992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data","metadata":{"execution":{"iopub.status.busy":"2024-10-30T15:49:09.097912Z","iopub.execute_input":"2024-10-30T15:49:09.0984Z","iopub.status.idle":"2024-10-30T15:49:09.194427Z","shell.execute_reply.started":"2024-10-30T15:49:09.09835Z","shell.execute_reply":"2024-10-30T15:49:09.193336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate the percentage of missing values for each column\nmissing_percentage = data.isnull().mean() * 100\n\n# Convert to a DataFrame for easier viewing, sorted by percentage\nmissing_percentage_data = missing_percentage.to_frame(name='Missing Percentage').sort_values(by='Missing Percentage', ascending=False)\n\n# Display the result\nprint(missing_percentage_data)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-30T15:49:09.19589Z","iopub.execute_input":"2024-10-30T15:49:09.196381Z","iopub.status.idle":"2024-10-30T15:49:09.224722Z","shell.execute_reply.started":"2024-10-30T15:49:09.196332Z","shell.execute_reply":"2024-10-30T15:49:09.223309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- The dependent variable, Monthly rental price is in string form and has duplicates, other variables need to be cleaned as well. \n- The vast majority of variables have missing values even going up to 90%, I would lose a lot of information as I decided to simply remove them. Filling so many missing values with the mean would possiby distort the actual distribution of the variables  ,so i settled with the following solution, variables whose were missing more than 85% percent of that and were barely correlated to the monthly rental price were first removed. The rest of the variables that contained missing values were filled with an iterative imputer. An iterative imputer is a method used to fill in missing data by modeling each feature with missing values as a function of other features, and then iteratively refining these estimates until convergence. This approach captures relationships between features, often resulting in more accurate imputations compared to simpler methods like mean or median imputation.","metadata":{}},{"cell_type":"markdown","source":"3. **Feature engineering**","metadata":{}},{"cell_type":"code","source":"import re\n\n\n# The column to clean is 'Monthly rental price'\n\n# Function to clean the price column\ndef clean_price(price_str):\n    # Check if the value is a string, if not return it as is (or handle missing values)\n    if isinstance(price_str, str):\n        # Remove euro symbol and any spaces\n        price_str = re.sub(r'[€\\s]', '', price_str)\n        \n        # Extract the first number (assumes format like '750750' or '12001200')\n        match = re.match(r'(\\d{1,3}(?:,\\d{3})?)', price_str)\n        \n        if match:\n            price_str = match.group(0)\n            # Convert comma to empty string for thousand separator\n            price_str = price_str.replace(',', '')\n            # Convert to numeric\n            return pd.to_numeric(price_str)\n        else:\n            return None  # Return None if no match found\n    else:\n        return price_str  # If it's not a string, return the value as is (e.g. NaN)\n\n# Apply the cleaning function ONLY to the 'Monthly rental price' column\ndata['Monthly rental price'] = data['Monthly rental price'].apply(clean_price)\n\n# Display the DataFrame with the cleaned 'Monthly rental price' column\n","metadata":{"execution":{"iopub.status.busy":"2024-10-30T15:49:09.228343Z","iopub.execute_input":"2024-10-30T15:49:09.228823Z","iopub.status.idle":"2024-10-30T15:49:09.312776Z","shell.execute_reply.started":"2024-10-30T15:49:09.228769Z","shell.execute_reply":"2024-10-30T15:49:09.311749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"columns_to_remove = [\n    \"Professional space\",\n    \"Living room\",\n    \"Basement\",\n    \"Primary energy consumption\",\n    \"Reference number of the EPC report\",\n    \"CO₂ emission\",\n    \"Yearly theoretical total energy consumption\",\n    \"Monthly costs\",\n    \n    \"As built plan\",\n    \"Address\",\n    \"Website\",\n    \"External reference\",\n    \"agency\",\n    \"living room\",\n    \"Dining room\",\n    \"Laundry room\",\n    \"Caretaker\",\n    \"Secure access / alarm\",\n    \"internet\",\n    \"Conformity certification for fuel tanks\",\n    \"Possible priority purchase right\",\n    \"Street frontage width\",\n    \"description\",\n    \"Basement surface\",\n    \"Neighbourhood or locality\",\n    \"How many fireplaces?\",\n    \"Terrace\",\n    \"Garden surface\",\n    \"Garden orientation\",\n    \"Planning permission obtained\",\n    \"Subdivision permit\",\n    \"Flood zone type\",\n    \"Surroundings type\",\n    \"Virtual visit\",\n    \"Property name\",\n    \"Proceedings for breach of planning regulations\",\n    \"Professional space surface\",\n    \"Obligation to build\",\n    \"Attic surface\",\n    \"Office surface\",\n    \"Attic\",\n    \"Surface of the plot\",\n    \"Connection to sewer network\",\n    \"Gas, water & electricity\",\n    \"Total ground floor buildable\",\n    \"Latest land use designation\",\n    \"Garden\",\n    \"price\",\n    \n    \"Tenement building\",\n    \"E-level (overall energy performance)\",\n    \"Agent's name\",\n    \"Land is facing street\",\n    \"Wooded land\",\n    \"Plot at rear\",\n    \"Flat land\",\n    \"Bedroom 4 surface\",\n    \"EPC description\",\n    \"Terrace orientation\",\n    \"Bedroom 5 surface\",\n    \"Width of the lot on the street\",\n    \"Isolated\",\n    \"Extra information\",\n    \"Available date\",\n    \"Number of annexes\"\n]\n\n# Remove the columns\ndf = data.drop(columns=columns_to_remove, errors='ignore')","metadata":{"execution":{"iopub.status.busy":"2024-10-30T15:49:09.314334Z","iopub.execute_input":"2024-10-30T15:49:09.314807Z","iopub.status.idle":"2024-10-30T15:49:09.330482Z","shell.execute_reply.started":"2024-10-30T15:49:09.314758Z","shell.execute_reply":"2024-10-30T15:49:09.329211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 1. Cleaning the 'space' column\ndef clean_space(space_entry):\n    if pd.isnull(space_entry):\n        return np.nan\n    # Extract the square meters using regex\n    match = re.search(r'(\\d+)\\s*m²', space_entry)\n    if match:\n        return int(match.group(1))  # Extract the numeric value\n    return np.nan\n\n# Apply the cleaning function to the 'space' column\ndf['total_space'] = df['space'].apply(clean_space)\n\n# 2. Cleaning the 'address' column\ndef clean_address(address_entry):\n    if pd.isnull(address_entry) or \"Ask for the exact address\" in address_entry:\n        return np.nan  # Treat these as missing values\n    # Extract postal codes (first four digits from the address)\n    match = re.search(r'\\b\\d{4}\\b', address_entry)\n    if match:\n        return int(match.group(0))  # Extract the postal code\n    return np.nan\n\n# Apply the cleaning function to the 'address' column\ndf['zip_code'] = df['address'].apply(clean_address)\n\n# Now, you can drop the original columns if necessary\ndf.drop(columns=['space', 'address'], inplace=True)\n\ndf","metadata":{"execution":{"iopub.status.busy":"2024-10-30T15:49:09.331996Z","iopub.execute_input":"2024-10-30T15:49:09.33243Z","iopub.status.idle":"2024-10-30T15:49:09.415368Z","shell.execute_reply.started":"2024-10-30T15:49:09.332378Z","shell.execute_reply":"2024-10-30T15:49:09.414106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I've written a custom cleaning process to extract usable data from the 'space' and 'address' columns by identifying square meter values and postal codes, respectively. The cleaned data is stored in new columns, and the original columns are removed once the transformation is complete.\n\n\n\n\n\n\n","metadata":{}},{"cell_type":"code","source":"\n# Function to extract numeric value from the string\ndef extract_numeric(value):\n    if pd.isna(value) or value == 'MISSING':\n        return value\n    return pd.to_numeric(value.split()[0], errors='coerce')\n\n# Apply the function to the 'Living area' column\ndf['Living area'] = df['Living area'].apply(extract_numeric)\n\n# Convert 'Living area' column to numeric, keeping 'MISSING' as is\ndf['Living area'] = pd.to_numeric(df['Living area'], errors='coerce').fillna(df['Living area'])\ndf = df.drop('Living area', axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-10-30T15:49:09.416864Z","iopub.execute_input":"2024-10-30T15:49:09.417343Z","iopub.status.idle":"2024-10-30T15:49:09.481792Z","shell.execute_reply.started":"2024-10-30T15:49:09.417274Z","shell.execute_reply":"2024-10-30T15:49:09.480694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The dataset underwent significant cleaning to address inconsistencies and prepare it for modeling. \nFirst, numeric values were extracted from mixed text entries, such as in the 'Living area' column, while preserving missing data. Binary categorical columns like 'Furnished' and 'Elevator' were converted from 'Yes/No' to 1/0 format. Surface area columns, including 'Terrace surface' and 'Bedroom 1 surface,' were cleaned by extracting only numeric values, leaving missing values unchanged. Finally, key categorical variables like 'Kitchen type' and 'Building condition' were label-encoded, while ensuring NaN values were retained.","metadata":{}},{"cell_type":"code","source":"# List of binary categorical columns\nbinary_columns = [\n    'Dressing room', 'Office', 'Small pet-friendly', 'Armored door', 'Heat pump',\n    'Photovoltaic solar panels', 'Thermic solar panels', 'Common water heater', \n    'Double glazing', 'Furnished', 'Intercom', 'Visio phone', 'Elevator', \n    'Accessible for disabled people', 'Air conditioning', 'TV cable', 'Jacuzzi',\n    'Sauna', 'Swimming pool', 'Internet'\n]\n\n# Assuming your DataFrame is called 'df'\n# Convert 'Yes'/'No' to 1/0 using a map function\nfor col in binary_columns:\n    df[col] = df[col].map({'Yes': 1, 'No': 0})\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-30T15:49:09.483082Z","iopub.execute_input":"2024-10-30T15:49:09.483452Z","iopub.status.idle":"2024-10-30T15:49:09.512256Z","shell.execute_reply.started":"2024-10-30T15:49:09.483415Z","shell.execute_reply":"2024-10-30T15:49:09.510971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# List of columns to clean\ncolumns_to_clean = ['Living room surface', 'Bedroom 1 surface', 'Bedroom 2 surface', \n                    'Terrace surface', 'Kitchen surface', 'Bedroom 3 surface']\n\n# Function to extract numeric part from the string\ndef extract_numeric(value):\n    if pd.isna(value):\n        return value  # Keep NaN as is\n    # Use regex to find any sequence of digits with optional decimal points\n    match = re.search(r'[\\d]+(?:\\.\\d+)?', str(value))\n    if match:\n        # Convert the extracted number to a float\n        return float(match.group(0))\n    else:\n        return pd.NA  # Return NaN if no numeric value found\n\n# Apply the function to the specified columns\ndf[columns_to_clean] = df[columns_to_clean].applymap(extract_numeric)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-30T15:49:09.513722Z","iopub.execute_input":"2024-10-30T15:49:09.51409Z","iopub.status.idle":"2024-10-30T15:49:09.559909Z","shell.execute_reply.started":"2024-10-30T15:49:09.514053Z","shell.execute_reply":"2024-10-30T15:49:09.558546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# List of columns to label encode\ncolumns_to_encode = ['Kitchen type', 'Energy class', 'Heating type', \n                     'Type of building', 'listing_type', 'Available as of','Building condition']\n\n# Initialize the LabelEncoder\nle = LabelEncoder()\n\n# Apply LabelEncoder and preserve NaN values\nfor column in columns_to_encode:\n    if column in df.columns:\n        # Save the NaN mask to remember which values were NaN\n        nan_mask = df[column].isna()\n        \n        # Convert non-NaN values to strings and apply LabelEncoder\n        df[column] = df[column].astype(str)\n        df[column] = le.fit_transform(df[column])\n        \n        # Restore the NaN values after encoding\n        df.loc[nan_mask, column] = pd.NA\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-30T15:49:09.561565Z","iopub.execute_input":"2024-10-30T15:49:09.561941Z","iopub.status.idle":"2024-10-30T15:49:09.592536Z","shell.execute_reply.started":"2024-10-30T15:49:09.561903Z","shell.execute_reply":"2024-10-30T15:49:09.59115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Step 1: Convert non-NaN values to integers and then to strings\ndf['zip_code'] = df['zip_code'].apply(lambda x: str(int(x)) if pd.notnull(x) else x)\n\n# Step 2: Ensure the column is of type 'object' (string)\ndf['zip_code'] = df['zip_code'].astype('object')","metadata":{"execution":{"iopub.status.busy":"2024-10-30T15:49:09.594224Z","iopub.execute_input":"2024-10-30T15:49:09.594695Z","iopub.status.idle":"2024-10-30T15:49:09.608996Z","shell.execute_reply.started":"2024-10-30T15:49:09.594654Z","shell.execute_reply":"2024-10-30T15:49:09.607503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The data was preprocessed using a combination of ordinal encoding and iterative imputation to handle missing values in both numeric and categorical columns. First, numeric and categorical columns were separated. Categorical variables were encoded using an OrdinalEncoder, transforming them into numeric format. For missing values in numeric columns, an IterativeImputer was used with a LGBMRegressor as the estimator, while for categorical columns, a LGBMClassifier imputed missing values. After imputation, the results were reassembled into DataFrames, and the categorical columns were inverse-transformed back to their original labels. Finally, the numeric and categorical columns were recombined to form the final dataset, with all missing values imputed and ready for modeling.","metadata":{}},{"cell_type":"code","source":"\nfrom lightgbm import LGBMRegressor, LGBMClassifier\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.preprocessing import OrdinalEncoder\n\n\n\n# Step 1: Separate numeric and categorical columns\nnumeric_columns = df.select_dtypes(include=['number']).columns.tolist()\ncategorical_columns = df.select_dtypes(exclude=['number']).columns.tolist()\n\n# Step 2: Ordinal encode categorical columns (convert strings to integers)\nencoder = OrdinalEncoder()\ndf[categorical_columns] = encoder.fit_transform(df[categorical_columns])\n\n# Step 3: Imputation for numeric columns using LGBMRegressor\nnumeric_imputer = IterativeImputer(estimator=LGBMRegressor(n_estimators=100, random_state=0,verbose=-1 ),\n                                   max_iter=10, random_state=0)\n\n# Step 4: Imputation for categorical columns using LGBMClassifier\ncategorical_imputer = IterativeImputer(estimator=LGBMClassifier(n_estimators=100, random_state=0,verbose=-1 ),\n                                       max_iter=10, random_state=0)\n\n# Step 5: Fit and transform each type of column separately\n# Impute numeric columns\nimputed_numeric_array = numeric_imputer.fit_transform(df[numeric_columns])\n\n# Impute categorical columns\nimputed_categorical_array = categorical_imputer.fit_transform(df[categorical_columns])\n\n# Step 6: Convert the results back to DataFrames\nimputed_numeric_df = pd.DataFrame(imputed_numeric_array, columns=numeric_columns)\nimputed_categorical_df = pd.DataFrame(imputed_categorical_array, columns=categorical_columns)\n\n# Step 7: Inverse transform categorical columns back to original categories\nimputed_categorical_df[categorical_columns] = encoder.inverse_transform(imputed_categorical_df)\n\n# Step 8: Combine the numeric and categorical columns back into a single DataFrame\nfinal_imputed_df = pd.concat([imputed_numeric_df, imputed_categorical_df], axis=1)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-30T15:49:09.611862Z","iopub.execute_input":"2024-10-30T15:49:09.612335Z","iopub.status.idle":"2024-10-30T15:52:29.867239Z","shell.execute_reply.started":"2024-10-30T15:49:09.612296Z","shell.execute_reply":"2024-10-30T15:52:29.866087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the binary columns based on your provided list\nbinary_columns = [\n    'Office', 'Small pet-friendly', 'Armored door', 'Heat pump','Thermic solar panels', 'Photovoltaic solar panels','Dressing room','Elevator',\n    'Thermic solar Panels', 'Common water heater', 'Double glazing', 'Furnished', 'Intercom',\n    'Visio phone', 'Accessible for disabled people', 'Air conditioning', 'TV cable', \n    'Jacuzzi', 'Sauna', 'Swimming pool', 'Internet'\n]\n\n# Step 1: Apply thresholding for binary columns in the DataFrame `final_imputed_df`\nfor col in binary_columns:\n    if col in final_imputed_df.columns:  # Ensure the column exists in the DataFrame\n        final_imputed_df[col] = np.where(final_imputed_df[col] < 0.5, 0, 1)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-30T15:52:29.872967Z","iopub.execute_input":"2024-10-30T15:52:29.873516Z","iopub.status.idle":"2024-10-30T15:52:29.891072Z","shell.execute_reply.started":"2024-10-30T15:52:29.873477Z","shell.execute_reply":"2024-10-30T15:52:29.889748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the multi-class numeric columns (discrete variables) that were imputed\nmulti_class_columns = [\n    'Kitchen type', 'Bedrooms', 'Bathrooms', 'Toilets', 'Heating type', \n    'Type of building', 'Number of floors', 'Number of frontages', \n    'Outdoor parking spaces', 'Construction year', 'Building condition', \n    'Available as of', 'Shower rooms', 'Covered parking spaces'\n]\n\n# Step 1: Apply rounding for all the multi-class numeric columns\nfor col in multi_class_columns:\n    if col in final_imputed_df.columns:  # Ensure the column exists in the DataFrame\n        # Round to nearest integer and convert to integer type\n        final_imputed_df[col] = np.round(final_imputed_df[col]).astype(int)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-30T15:52:29.89263Z","iopub.execute_input":"2024-10-30T15:52:29.893027Z","iopub.status.idle":"2024-10-30T15:52:29.90769Z","shell.execute_reply.started":"2024-10-30T15:52:29.892988Z","shell.execute_reply":"2024-10-30T15:52:29.906387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# List of columns to one-hot encode\ncolumns_to_encode = [\n    'Office', 'Small pet-friendly', 'Armored door', 'Heat pump', 'Photovoltaic solar panels',\n    'Thermic solar panels', 'Common water heater', 'Double glazing', 'Furnished', 'Intercom',\n    'Visio phone', 'Accessible for disabled people', 'Air conditioning', 'TV cable', \n    'Jacuzzi', 'Sauna', 'Swimming pool', 'Internet', 'zip_code'\n]\n\n# Step 1: Apply one-hot encoding to the specified columns\nfinal_encoded_df = pd.get_dummies(final_imputed_df, columns=columns_to_encode, drop_first=True)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-30T15:52:29.90953Z","iopub.execute_input":"2024-10-30T15:52:29.910668Z","iopub.status.idle":"2024-10-30T15:52:29.941238Z","shell.execute_reply.started":"2024-10-30T15:52:29.910617Z","shell.execute_reply":"2024-10-30T15:52:29.93965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This code performs two key data preprocessing steps: first, it transforms a set of specified columns containing float values into strict binary features (0 or 1) using a 0.5 threshold, ensuring features like 'Office', 'Swimming pool', and other amenities are represented as clear binary indicators; then, it applies one-hot encoding to these binary features along with the 'cleaned_address' categorical column , resulting in a transformed dataset where all categorical variables are properly encoded for machine learning model consumption","metadata":{}},{"cell_type":"markdown","source":"4. **Predictive modeling**","metadata":{}},{"cell_type":"markdown","source":"This code implements a machine learning pipeline for predicting monthly rental prices: it splits the dataset into features (X) and target variable (y), creates training and test sets, then uses LightGBM (a gradient boosting framework) with GridSearchCV to automatically find the best combination of hyperparameters (learning rate and max depth) through 5-fold cross-validation, ultimately training a model and evaluating its performance on the test set.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split, GridSearchCV\nfrom lightgbm import LGBMRegressor\n\n\n\n# Step 1: Separate the features (X) and the target (y)\nX = final_encoded_df.drop(columns=['Monthly rental price'])\ny = final_encoded_df['Monthly rental price']\n\n# Step 2: Split data into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\n# Step 3: Train a LightGBM regression model with GridSearchCV for hyperparameter tuning\nestimator = LGBMRegressor(random_state=0,verbose=-1)#\n\n\n\n\n# Define a parameter grid to search over\nparam_grid = {\n    \"learning_rate\": [0.1, 0.05, 0.01],\n    \"max_depth\": [3, 5, 10]\n}\n\n# Use GridSearchCV to search for the best parameters\nsearch = GridSearchCV(estimator, param_grid, n_jobs=-1, cv=5)  # 5-fold cross-validation\nsearch.fit(x_train, y_train)\n\n# Step 4: Display the best parameters and evaluate model performance\nprint(\"Best estimator: \", search.best_params_)\nprint(\"Test set score: \", search.best_estimator_.score(x_test, y_test))","metadata":{"execution":{"iopub.status.busy":"2024-10-30T15:54:53.221503Z","iopub.execute_input":"2024-10-30T15:54:53.22196Z","iopub.status.idle":"2024-10-30T15:54:59.997517Z","shell.execute_reply.started":"2024-10-30T15:54:53.221918Z","shell.execute_reply":"2024-10-30T15:54:59.996244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This code implements a machine learning pipeline using LightGBM to predict monthly rent prices, followed by an explanation of the model's predictions using SHAP (SHapley Additive exPlanations) values. The process involves training a LightGBM model with optimized parameters found through GridSearchCV, running for 100 boosting rounds on the training data. Then, a SHAP TreeExplainer is created to analyze how each feature contributes to the model's predictions, generating SHAP values for the test dataset. The resulting summary plot displays these feature impacts, where each point represents a sample, colored by the feature value (blue for low, red for high), and positioned on the x-axis according to its SHAP value (impact on prediction). The visualization reveals that 'total_space' has the strongest influence on rent predictions, followed by features like 'Toilets' and 'Terrace surface', while features such as 'Sauna_1' and 'Bedroom 3 surface' show minimal impact on the predicted rent prices.","metadata":{}},{"cell_type":"code","source":"import shap\nimport lightgbm as lgb\n\n\n\n# Convert the data to LightGBM Dataset format\nlgb_train = lgb.Dataset(x_train, y_train)\nlgb_test = lgb.Dataset(x_test, y_test, reference=lgb_train)\n\n# Get the best parameters from GridSearchCV\nbest_params = search.best_params_\n\n# Create and train a new model with the best parameters\nmodel = lgb.train(best_params, lgb_train, num_boost_round=100)\n\n# Now create the SHAP explainer\nexplainer = shap.TreeExplainer(model)\n\n# Calculate SHAP values\nshap_values = explainer.shap_values(x_test)\n\n# Visualize the results\nshap.summary_plot(shap_values, x_test)","metadata":{"execution":{"iopub.status.busy":"2024-10-30T15:55:31.466124Z","iopub.execute_input":"2024-10-30T15:55:31.467195Z","iopub.status.idle":"2024-10-30T15:55:33.59826Z","shell.execute_reply.started":"2024-10-30T15:55:31.467147Z","shell.execute_reply":"2024-10-30T15:55:33.596609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"pip install econml","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-10-30T15:55:49.088655Z","iopub.execute_input":"2024-10-30T15:55:49.089079Z","iopub.status.idle":"2024-10-30T15:56:06.035591Z","shell.execute_reply.started":"2024-10-30T15:55:49.089041Z","shell.execute_reply":"2024-10-30T15:56:06.033426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"5. **Causal modeling**","metadata":{}},{"cell_type":"code","source":"from econml.solutions.causal_analysis import CausalAnalysis\n\n# List of categorical features (add any other categorical features from your dataset)\ncategorical = [\n    'Furnished_1', 'listing_type', 'Heating type', 'Type of building', \n    'Armored door_1', 'Elevator', 'Small pet-friendly_1', 'Office_1','Sauna_1', \n    'Accessible for disabled people_1', 'Swimming pool_1', 'Internet_1','Heat pump_1', 'Kitchen type','Double glazing_1','zip_code_1030', 'zip_code_1040', 'zip_code_1050', 'zip_code_1060', 'zip_code_1070', 'zip_code_1080', 'zip_code_1081', 'zip_code_1082', 'zip_code_1083', 'zip_code_1090', 'zip_code_1120', 'zip_code_1140', 'zip_code_1150', 'zip_code_1160', 'zip_code_1170', 'zip_code_1180', 'zip_code_1190', 'zip_code_1200', 'zip_code_1210', 'zip_code_1700', 'zip_code_2800', 'zip_code_8500', 'zip_code_9300', 'zip_code_9660', 'zip_code_9700','Outdoor parking spaces','Common water heater_1','Thermic solar panels_1','Photovoltaic solar panels_1', 'Air conditioning_1'\n    # Add more as needed\n]\n\n# Heterogeneity features (features where treatment effects might vary across subgroups)\nhetero_cols = ['Energy class', 'Building condition', 'Construction year']\n\n# Initialize causal analysis\nca = CausalAnalysis(\n    feature_inds=['Floor', 'Bedrooms', 'Bathrooms', 'Toilets','Number of frontages', \n                  'Terrace surface',  \n                  'total_space','Outdoor parking spaces', ]+categorical ,\n    categorical=categorical,\n    heterogeneity_inds=hetero_cols,\n    classification=False,\n    nuisance_models=\"automl\",\n    heterogeneity_model=\"linear\",\n    n_jobs=-1,\n    random_state=123,\n    upper_bound_on_cat_expansion=6\n)\n\n# Fit the model\n# Fit the causal analysis model\nca.fit(x_train, y_train)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-10-30T15:56:06.037929Z","iopub.execute_input":"2024-10-30T15:56:06.038423Z","iopub.status.idle":"2024-10-30T16:16:17.567935Z","shell.execute_reply.started":"2024-10-30T15:56:06.038376Z","shell.execute_reply":"2024-10-30T16:16:17.56612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In essence the code is ranking features by their causal importance to the outcome, based on the p-value of the causal effect. Features with smaller p-values are more likely to have a significant impact on the target variable, providing valuable insights into which factors are the most influential in your causal analysis. We can already spot a difference in feature importance in the causal model in comparison to the predictive model. Zip codes tend to have bigger causal impact on prices than would be assumed from the predictive model for example.","metadata":{}},{"cell_type":"code","source":"# get global causal effect ordered by causal importance (pvalue)\nglobal_summ = ca.global_causal_effect(alpha=0.05)\nglobal_summ.sort_values(by=\"p_value\")","metadata":{"execution":{"iopub.status.busy":"2024-10-30T16:18:31.116947Z","iopub.execute_input":"2024-10-30T16:18:31.117503Z","iopub.status.idle":"2024-10-30T16:18:31.180263Z","shell.execute_reply.started":"2024-10-30T16:18:31.11746Z","shell.execute_reply":"2024-10-30T16:18:31.179134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This helper function effectively displays the causal effect size and significance of each feature. Features with significant effects (indicated by asterisks) stand out, making this an informative visualization for understanding which features have a statistically meaningful impact on the outcome.","metadata":{}},{"cell_type":"code","source":"# helper function to plot error bar\ndef errorbar(res):\n    xticks = res.index.get_level_values(0)\n    lowererr = res[\"point\"] - res[\"ci_lower\"]\n    uppererr = res[\"ci_upper\"] - res[\"point\"]\n    xticks = [\n        \"{}***\".format(t)\n        if p < 1e-6\n        else (\"{}**\".format(t) if p < 1e-3 else (\"{}*\".format(t) if p < 1e-2 else t))\n        for t, p in zip(xticks, res[\"p_value\"])\n    ]\n    plot_title = \"Direct Causal Effect of Each Feature with 95% Confidence Interval, \"\n    plt.figure(figsize=(15, 5))\n    plt.errorbar(\n        np.arange(len(xticks)),\n        res[\"point\"],\n        yerr=[lowererr, uppererr],\n        fmt=\"o\",\n        capsize=5,\n        capthick=1,\n        barsabove=True,\n    )\n    plt.xticks(np.arange(len(xticks)), xticks, rotation=45)\n    plt.title(plot_title)\n    plt.axhline(0, color=\"r\", linestyle=\"--\", alpha=0.5)\n    plt.ylabel(\"Average Treatment Effect\")","metadata":{"execution":{"iopub.status.busy":"2024-10-30T16:18:41.84507Z","iopub.execute_input":"2024-10-30T16:18:41.845528Z","iopub.status.idle":"2024-10-30T16:18:41.854979Z","shell.execute_reply.started":"2024-10-30T16:18:41.845488Z","shell.execute_reply":"2024-10-30T16:18:41.853668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"errorbar(global_summ)","metadata":{"execution":{"iopub.status.busy":"2024-10-30T16:18:47.252859Z","iopub.execute_input":"2024-10-30T16:18:47.253303Z","iopub.status.idle":"2024-10-30T16:18:47.913426Z","shell.execute_reply.started":"2024-10-30T16:18:47.253243Z","shell.execute_reply":"2024-10-30T16:18:47.912189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" In the following section, we are going to use the presence of a thermic solar pan as an example to learn how different type of appartments  may increase the monthly rental price with the addition of a solar panel by looking a decision tree. \n \n \"Conditional average treatment effect ( CATE ) is a crucial concept in causal inference. It focuses on estimating treatment effects for specific subgroups or individuals, allowing for more personalized interventions by accounting for heterogeneity across different subpopulations.\"\n\nThe strongest treatment effects (highest CATE means) appear in newer properties (post-1955) with better conditions\nProperties in poor condition show more varied effects (higher standard deviations)\nThe most recent properties (built after 1992) show the highest positive treatment effect (41.922)\n\nThis suggests that the causal impact  varies significantly based on the property's age and condition, with newer properties generally showing stronger positive effects on rental prices.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12, 8))\nca.plot_heterogeneity_tree(\n    x_test,\n    \"Thermic solar panels_1\",\n    max_depth=2,\n    min_impurity_decrease=1e-6,\n    min_samples_leaf = 5\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-30T16:19:06.533785Z","iopub.execute_input":"2024-10-30T16:19:06.534195Z","iopub.status.idle":"2024-10-30T16:19:07.109263Z","shell.execute_reply.started":"2024-10-30T16:19:06.534158Z","shell.execute_reply":"2024-10-30T16:19:07.108088Z"},"trusted":true},"execution_count":null,"outputs":[]}]}